{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Обработка числовых и категориальных признаков"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Масштабирование признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Создать признак\n",
    "X_data = np.array([[-1.0], [-1.5], [2.0], [3.5], [8.0],\n",
    "                   [5.0], [-2.0], [2.5], [3.5], [6.0]])\n",
    "\n",
    "# Создание экземпляра класса MinMaxScaler(), диапазон {0,1}\n",
    "minmax_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "# Масштабирование признака\n",
    "X_data_scaled = minmax_scaler.fit_transform(X_data)\n",
    "\n",
    "# Данные до и после масштабирования\n",
    "print('До масштабирования:   ', X_data[:5].T)\n",
    "print('После масштабирования:',  X_data_scaled[:5].T)\n",
    "\n",
    "# Min и Max значения до и после масштабирования\n",
    "print('min:', X_data.min(), 'max:', X_data.max())\n",
    "print('min:', X_data_scaled.min(), 'max:', X_data_scaled.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Масштабирование данных, имеющих сильные выбросы\n",
    "# Создать признак\n",
    "X_data = np.array([[-100.0], [5.0], [2.0], [-3.5], [5.0],\n",
    "                   [8.0], [-20.0], [3.0], [3.5], [60.0]])\n",
    "\n",
    "# Создание экземпляра класса RobustScaler()\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Масштабирование признака\n",
    "X_data_robust = robust_scaler.fit_transform(X_data)\n",
    "\n",
    "# Данные до и после масштабирования\n",
    "print('До масштабирования:   ', X_data[:5].T)\n",
    "print('После масштабирования:',  X_data_scaled[:5].T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Стандартизация признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Создать признак\n",
    "X_data = np.array([[10.0], [-1.5], [2.0], [3.0], [4.0],\n",
    "                   [3.0], [-5.0], [2.5], [3.5], [2.0]])\n",
    "\n",
    "# Создание экземпляра класса StandardScaler()\n",
    "standard = preprocessing.StandardScaler()\n",
    "\n",
    "# Стандартизация признака признака\n",
    "X_data_standard = standard.fit_transform(X_data)\n",
    "\n",
    "# Данные до и после стандартизации\n",
    "print('До стандартизации:   ', X_data[:5].T)\n",
    "print('После стандартизации:',  X_data_scaled[:5].T)\n",
    "\n",
    "# Данные до и после стандартизации\n",
    "print('Среднее:', round(X_data_standard.mean()))\n",
    "print('Стандартное отклонение:', X_data_standard.std())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Нормализация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Матрица признаков\n",
    "X_data = np.array([[0.7,  0.5],\n",
    "                   [1.5,  2.8],\n",
    "                   [1.5,  20.2],\n",
    "                   [2.5,  26.4], \n",
    "                   [8.2,  6.3]])\n",
    "\n",
    "# Создание экземпляра класса Normalizer()\n",
    "# L-квадрат норма (евклидова норма)\n",
    "# L^2 = sqrt(x1^2 + x2^2 ... xn^2) = 1\n",
    "\n",
    "normalizer = Normalizer(norm=\"l2\")\n",
    "X_norm_l2 = normalizer.transform(X_data)\n",
    "\n",
    "# Преобразовать матрицу признаков\n",
    "print(X_norm_l2)\n",
    "\n",
    "# L' норма (манхэтеннская норма)\n",
    "# L' = x1 + x2 ... xn = 1\n",
    "\n",
    "normalizer = Normalizer(norm=\"l1\")\n",
    "X_norm_l1 = normalizer.transform(X_data)\n",
    "\n",
    "# Преобразовать матрицу признаков\n",
    "print(X_norm_l1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Генерация полиномиальных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Матрица признаков\n",
    "X_data = np.array([[0, 1],\n",
    "                   [2, 3],\n",
    "                   [4, 5]])\n",
    "\n",
    "# Создание экземпляра класса PolynomialFeatures\n",
    "polynom_data = PolynomialFeatures(degree=2, interaction_only=False)\n",
    "\n",
    "# Создать новую матрицу признаков\n",
    "polynom_data.fit_transform(X_data)\n",
    "\n",
    "# out -> [1, x1, x2, x1^2, x1*x2, x2^2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Преобразование признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Матрица признаков\n",
    "X_data = np.array([[0, 1, 3],\n",
    "                   [3, 4, 5],\n",
    "                   [10, 7, 8],\n",
    "                   [2, 5, 10]])\n",
    "\n",
    "# Функция для обработки данных\n",
    "def transform_func(row):\n",
    "    return row**2\n",
    "\n",
    "# Создание экземпляра класса FunctionTransformer()\n",
    "transform_data = FunctionTransformer(transform_func)\n",
    "\n",
    "# Создать новую матрицу признаков\n",
    "transform_data.transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание датафрейма и использование метода apply()\n",
    "df = pd.DataFrame(X_data)\n",
    "\n",
    "df_new = df.apply(transform_func)\n",
    "df_new.rename(columns=lambda x: 'feature_' + str(x), inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6. Анализ выбросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Использование метода describe()\n",
    "df_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод 25-го и 75-го перцентиля данных\n",
    "df_new.describe()['feature_0'][['25%','75%']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = df_new.feature_0.quantile(0.25)  # 25-й перцентиль\n",
    "q3 = df_new.feature_0.quantile(0.75)  # 75-й перцентиль\n",
    "iqr = q3 - q1                         # межквартильный размах\n",
    "lower_bound = q1 - (iqr*1.5)          # нижняя граница выбросов\n",
    "upper_bound = q3 + (iqr*1.5)          # верхняя граница выбросов\n",
    " \n",
    "print('25-й перцентиль: {},'.format(q1),\n",
    "      '75-й перцентиль: {},'.format(q3),\n",
    "      \"IQR: {}, \".format(iqr),\n",
    "      \"Границы выбросов: [{lb}, {ub}].\".format(lb=lower_bound, ub=upper_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_value = df_new['feature_0'].mean()\n",
    "\n",
    "def del_outliers(row):\n",
    "    if row > upper_bound or row < lower_bound:\n",
    "        return mean_value\n",
    "    else:\n",
    "        return row\n",
    "    \n",
    "df_new['feature_3'] = df_new['feature_0'].apply(del_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "norm_rv = stats.norm(loc=30, scale=5)\n",
    "samples = np.trunc(norm_rv.rvs(365))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n",
    "fig.suptitle('Распределение признака feature_0')\n",
    "histplot = sns.histplot(x=samples, ax=axes[0])\n",
    "boxplot = sns.boxplot(x=samples, ax=axes[1])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n",
    "fig.suptitle('Распределение признака feature_3')\n",
    "histplot = sns.histplot(x=samples, ax=axes[0])\n",
    "boxplot = sns.boxplot(x=samples, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание признака на основе булева условия\n",
    "df_new['feature_4'] = np.where(df_new['feature_1'] < 20,  0,  1)\n",
    "\n",
    "# Логарифмирование признака\n",
    "df_new['feature_5'] = [np.log(х) for х in df_new['feature_2']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7. Дискретизация признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Создать признак\n",
    "X_data = np.array([[1], [10], [100], [1000]])\n",
    "\n",
    "# Создание экземпляра класса Binarizer()\n",
    "binarizer = Binarizer(50)                # порого бинаризации = 50\n",
    "\n",
    "# Бинаризация признака\n",
    "X_binar = binarizer.fit_transform(X_data)\n",
    "print(X_binar.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиение на диапазоны\n",
    "X_binar_digit = np.digitize(X_data, bins=[1, 10, 100, 1000])\n",
    "print(X_binar_digit.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8. Работа с пропущенными данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Создание списка с пропущенными данными\n",
    "numbers = [[1, 8, np.nan], \n",
    "           [2, np.nan, 16], \n",
    "           [np.nan, 10, 17],\n",
    "           [7, np.nan, 18], \n",
    "           [5, 12, np.nan]] \n",
    " \n",
    "header = ['feature_1', 'feature_2', 'feature_3'] \n",
    "\n",
    "# Создание датафрейма\n",
    "df = pd.DataFrame(data = numbers, columns = header)\n",
    "\n",
    "# Подсчёт пропущенных значений в датафрейме\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация пропусков\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(df.isna().transpose(),\n",
    "            cmap=\"YlGnBu\",\n",
    "            cbar_kws={'label': 'Пропущенные данные'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Замена пропусков\n",
    "\n",
    "feat_1_mean = df['feature_1'].mean()                  # расчёт среднего\n",
    "feat_2_median = df['feature_2'].median()              # расчёт медианы\n",
    "feat_3_mode = df['feature_3'].mode()                  # расчёт моды\n",
    "\n",
    "df['feature_1'].fillna(feat_1_mean, inplace=True)     # замена пропусков на среднее\n",
    "df['feature_2'].fillna(feat_2_median, inplace=True)   # замена пропусков на медиану\n",
    "df['feature_3'].fillna(feat_3_mode[0], inplace=True)  # замена пропусков на моду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импутация признаков\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X_data = np.array(numbers)\n",
    "\n",
    "# Создание экземпляра класса SimpleImputer()\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "# Импутация значений\n",
    "mean_imputed = mean_imputer.fit_transform(X_data)\n",
    "\n",
    "print(\"Импутированное значение:\",  mean_imputed[0,2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9. Кодирование номинальных категориальных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
    "\n",
    "# Создание признака\n",
    "X_data = np.array([['pressure'],\n",
    "                   ['gas'],\n",
    "                   ['pressure'],\n",
    "                   ['temp'],\n",
    "                   ['pressure']])\n",
    "\n",
    "# Создание экземпляра класса LabelBinarizer()\n",
    "one_hot = LabelBinarizer()\n",
    "\n",
    "# Кодирование признака\n",
    "one_hot.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание мультипризнака\n",
    "X_data_mult = np.array([['pressure','humidity'],\n",
    "                        ['gas','pressure', 'temp'],\n",
    "                        ['pressure','humidity'],\n",
    "                        ['temp','pressure'],\n",
    "                        ['pressure','humidity']])\n",
    "\n",
    "# Создание экземпляра класса MultiLabelBinarizer()\n",
    "one_hot = MultiLabelBinarizer()\n",
    "\n",
    "# Кодирование признака\n",
    "one_hot.fit_transform(X_data_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодирование признака и создание датафрейма\n",
    "pd.get_dummies(X_data[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Создание словаря\n",
    "data = {'temp': ['high', 'low', 'high', 'low'], \n",
    "        'size': ['small', 'big', 'big', 'medium'], \n",
    "        'pressure': ['isobar','isochor','isother','isochor']}\n",
    "\n",
    "# Создание датафрейма\n",
    "df = pd.DataFrame(data = data)\n",
    "\n",
    "# Создание экземпляра класса OrdinalEncoder()\n",
    "ord_enc = OrdinalEncoder()\n",
    "\n",
    "# Кодирование признака\n",
    "df['pressure_ordinal'] = ord_enc.fit_transform(df[['pressure']])\n",
    "\n",
    "# Создание экземпляра класса LabelEncoder()\n",
    "lab_enc = LabelEncoder()\n",
    "\n",
    "# Кодирование признака\n",
    "df['pressure_label'] = lab_enc.fit_transform(df['pressure'])\n",
    "\n",
    "# Создание экземпляра класса OneHotEncoder()\n",
    "ohe_enc = OneHotEncoder()\n",
    "# Кодирование признака\n",
    "ohe_res = ohe_enc.fit_transform(df[['pressure']])\n",
    "\n",
    "ohe_df = pd.DataFrame(ohe_res.toarray(), columns=list(*ohe_enc.categories_))\n",
    "df = pd.concat([df, ohe_df], axis=1)\n",
    "\n",
    "# OHE с использованием библиотеки pandas\n",
    "pd.get_dummies(df, columns=['size','temp'], prefix=['size','temp'])\n",
    "\n",
    "# Проверка типа данных\n",
    "type(df['pressure_ordinal'].iloc[2])\n",
    "type(df['pressure_label'].iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "# Создание экземпляра класса OrdinalEncoder()\n",
    "ord_enc = ce.OrdinalEncoder()\n",
    "data_ord = ord_enc.fit_transform(df['size'])\n",
    "df = pd.concat([df, data_ord], axis=1)\n",
    "\n",
    "# Создание экземпляра класса OneHotEncoder()\n",
    "ohe_enc = ce.OneHotEncoder(cols=['pressure'])\n",
    "data_ohe = ohe_enc.fit_transform(df['pressure'])\n",
    "df = pd.concat([df, data_ohe], axis=1)\n",
    "\n",
    "# Создание экземпляра класса BinaryEncoder()\n",
    "bin_enc = ce.BinaryEncoder(cols=['pressure'])\n",
    "data_bin = bin_enc.fit_transform(data['pressure'])\n",
    "df = pd.concat([df, data_bin], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10. Кодирование порядковых категориальных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Замена порядковых категориальных признаков методом replace()\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "# Создание словаря\n",
    "data = {'temp': ['high', 'low', 'high', 'middle'], \n",
    "        'size': ['small', 'big', 'big', 'medium'], \n",
    "        'pressure': ['isobar','isochor','isother','isochor']}\n",
    "\n",
    "# Создание датафрейма\n",
    "df = pd.DataFrame(data = data)\n",
    "\n",
    "scale_rep =  {'small': 1, 'medium': 2, 'big': 3}\n",
    "df['size'] = df['size'].replace(scale_rep)\n",
    "\n",
    "# Замена признаков с использованием вложенных словарей\n",
    "scale_rdic =  {'temp': {'low': 1, 'middle': 2, 'high': 3}}\n",
    "df = df.replace(scale_rdic)\n",
    "\n",
    "# Замена порядковых категориальных признаков методом map()\n",
    "scale_map =  {'isochor': 1, 'isother': 2, 'isobar': 3}\n",
    "df['pressure'] = df['pressure'].map(scale_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.11. Кодирование словарей признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание массива категориальных признаков на основе словарей\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Создание словаря с данными\n",
    "data_dict =  [{'high': 2,  'low':  5},\n",
    "              {'high': 4,  'low':  3},\n",
    "              {'high': 3,  'middle': 2},\n",
    "              {'high': 2,  'middle': 4}]\n",
    "               \n",
    "# Создание экземпляра класса DictVectorizer()\n",
    "dictvec = DictVectorizer(sparse=False)\n",
    "\n",
    "# Кодировка признаков\n",
    "features = dictvec.fit_transform(data_dict)\n",
    "\n",
    "# Матрица признаков и названия признаков\n",
    "feature_names = dictvec.get_feature_names()\n",
    "\n",
    "# если sparse=True, то features.toarray()\n",
    "features, feature_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.12. Категориальные переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Создание датафрейма на основе генерации случайных чисел\n",
    "df = pd.DataFrame({'value': np.random.randint(0, 100, 1000)})\n",
    "\n",
    "# Создание меток категориий\n",
    "labels = ['{0} - {1}'.format(i, i + 9) for i in range(0, 100, 10)]\n",
    "# Создание признака категорий\n",
    "df['value_group'] = pd.cut(df.value, range(0, 105, 10), right=False, labels=labels)\n",
    "\n",
    "sns.countplot(x='value_group', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ОБЛАКО СЛОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним в функцию способ визуализации слов, он еще пригодится:\n",
    "def show_wordcloud(data, background_color, colormap):\n",
    "    \"\"\"Рисуем облако слов с заданными параметрами.\"\"\"\n",
    "    wordcloud = WordCloud(\n",
    "        background_color = background_color,\n",
    "        colormap = colormap,\n",
    "        max_font_size = 40,\n",
    "        max_words=100,\n",
    "        scale = 3,\n",
    "        random_state = 42\n",
    "    ).generate(str(data))\n",
    "\n",
    "    plt.figure(1, figsize = (20, 20))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "print('Визуализируем самые популярные теги набора данных:')\n",
    "\n",
    "# чтобы увидеть на визуализации именно слитные Тэги, а не отдельные слова, \n",
    "# преобразуем строки данных в блоки для каждого тега:\n",
    "def get_tags_string(tags_data):\n",
    "    \"\"\"Соединяем все слова кажого тега нижним подчеркиванием.\"\"\"\n",
    "    tags_data = tags_data.replace('[', '').replace(']', '').replace(\"'\", '')\n",
    "    tags_list = tags_data.split(',')\n",
    "\n",
    "    tags_string = ''\n",
    "    for teg in tags_list:\n",
    "        teg = teg.strip().replace(' ', '_')\n",
    "        tags_string = tags_string + teg + ' '\n",
    "\n",
    "    return tags_string\n",
    "\n",
    "# рисуем диаграмму:\n",
    "cloud = hotels['tags'].apply(get_tags_string)\n",
    "show_wordcloud(cloud, 'white', 'viridis')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1r8lSGTjfjjZ89JHyUQsqyHE2RdPqW_FA?usp=sharing\n",
    "решение ментора"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
