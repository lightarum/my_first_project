{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-7. Прогнозирование биологического ответа (HW-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цель: Необходимо предсказать биологический ответ молекул (столбец 'Activity') по их химическому составу (столбцы D1-D1776)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sveta\\AppData\\Local\\Temp\\ipykernel_11448\\631027550.py:22: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    }
   ],
   "source": [
    "#импорт библиотек\n",
    "import numpy as np #для матричных вычислений\n",
    "import pandas as pd #для анализа и предобработки данных\n",
    "import matplotlib.pyplot as plt #для визуализации\n",
    "import seaborn as sns #для визуализации\n",
    "\n",
    "from sklearn import linear_model #линейные моделиё\n",
    "from sklearn import tree #деревья решений\n",
    "from sklearn import ensemble #ансамбли\n",
    "from sklearn import metrics #метрики\n",
    "from sklearn import preprocessing #предобработка\n",
    "from sklearn.model_selection import train_test_split #сплитование выборки\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import hyperopt\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "import optuna\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Знакомство с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>D1767</th>\n",
       "      <th>D1768</th>\n",
       "      <th>D1769</th>\n",
       "      <th>D1770</th>\n",
       "      <th>D1771</th>\n",
       "      <th>D1772</th>\n",
       "      <th>D1773</th>\n",
       "      <th>D1774</th>\n",
       "      <th>D1775</th>\n",
       "      <th>D1776</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.497009</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132956</td>\n",
       "      <td>0.678031</td>\n",
       "      <td>0.273166</td>\n",
       "      <td>0.585445</td>\n",
       "      <td>0.743663</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.606291</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111209</td>\n",
       "      <td>0.803455</td>\n",
       "      <td>0.106105</td>\n",
       "      <td>0.411754</td>\n",
       "      <td>0.836582</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.480124</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209791</td>\n",
       "      <td>0.610350</td>\n",
       "      <td>0.356453</td>\n",
       "      <td>0.517720</td>\n",
       "      <td>0.679051</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538825</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.196344</td>\n",
       "      <td>0.724230</td>\n",
       "      <td>0.235606</td>\n",
       "      <td>0.288764</td>\n",
       "      <td>0.805110</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.517794</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494734</td>\n",
       "      <td>0.781422</td>\n",
       "      <td>0.154361</td>\n",
       "      <td>0.303809</td>\n",
       "      <td>0.812646</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Activity        D1        D2    D3   D4        D5        D6        D7  \\\n",
       "0         1  0.000000  0.497009  0.10  0.0  0.132956  0.678031  0.273166   \n",
       "1         1  0.366667  0.606291  0.05  0.0  0.111209  0.803455  0.106105   \n",
       "2         1  0.033300  0.480124  0.00  0.0  0.209791  0.610350  0.356453   \n",
       "3         1  0.000000  0.538825  0.00  0.5  0.196344  0.724230  0.235606   \n",
       "4         0  0.100000  0.517794  0.00  0.0  0.494734  0.781422  0.154361   \n",
       "\n",
       "         D8        D9  ...  D1767  D1768  D1769  D1770  D1771  D1772  D1773  \\\n",
       "0  0.585445  0.743663  ...      0      0      0      0      0      0      0   \n",
       "1  0.411754  0.836582  ...      1      1      1      1      0      1      0   \n",
       "2  0.517720  0.679051  ...      0      0      0      0      0      0      0   \n",
       "3  0.288764  0.805110  ...      0      0      0      0      0      0      0   \n",
       "4  0.303809  0.812646  ...      0      0      0      0      0      0      0   \n",
       "\n",
       "   D1774  D1775  D1776  \n",
       "0      0      0      0  \n",
       "1      0      1      0  \n",
       "2      0      0      0  \n",
       "3      0      0      0  \n",
       "4      0      0      0  \n",
       "\n",
       "[5 rows x 1777 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('_train_sem09 (1).csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Activity    0\n",
       "D1          0\n",
       "D2          0\n",
       "D3          0\n",
       "D4          0\n",
       "           ..\n",
       "D1772       0\n",
       "D1773       0\n",
       "D1774       0\n",
       "D1775       0\n",
       "D1776       0\n",
       "Length: 1777, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверяем наличие пропусков\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3751 entries, 0 to 3750\n",
      "Columns: 1777 entries, Activity to D1776\n",
      "dtypes: float64(942), int64(835)\n",
      "memory usage: 50.9 MB\n"
     ]
    }
   ],
   "source": [
    "# изучаем данные\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>D1767</th>\n",
       "      <th>D1768</th>\n",
       "      <th>D1769</th>\n",
       "      <th>D1770</th>\n",
       "      <th>D1771</th>\n",
       "      <th>D1772</th>\n",
       "      <th>D1773</th>\n",
       "      <th>D1774</th>\n",
       "      <th>D1775</th>\n",
       "      <th>D1776</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.542255</td>\n",
       "      <td>0.076948</td>\n",
       "      <td>0.592436</td>\n",
       "      <td>0.068142</td>\n",
       "      <td>0.038990</td>\n",
       "      <td>0.212112</td>\n",
       "      <td>0.686653</td>\n",
       "      <td>0.274713</td>\n",
       "      <td>0.455133</td>\n",
       "      <td>0.749517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026926</td>\n",
       "      <td>0.014663</td>\n",
       "      <td>0.013863</td>\n",
       "      <td>0.021861</td>\n",
       "      <td>0.015196</td>\n",
       "      <td>0.016796</td>\n",
       "      <td>0.012263</td>\n",
       "      <td>0.011730</td>\n",
       "      <td>0.020261</td>\n",
       "      <td>0.011197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.498278</td>\n",
       "      <td>0.079989</td>\n",
       "      <td>0.105860</td>\n",
       "      <td>0.078414</td>\n",
       "      <td>0.115885</td>\n",
       "      <td>0.102592</td>\n",
       "      <td>0.078702</td>\n",
       "      <td>0.090017</td>\n",
       "      <td>0.162731</td>\n",
       "      <td>0.071702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161889</td>\n",
       "      <td>0.120215</td>\n",
       "      <td>0.116938</td>\n",
       "      <td>0.146249</td>\n",
       "      <td>0.122348</td>\n",
       "      <td>0.128522</td>\n",
       "      <td>0.110074</td>\n",
       "      <td>0.107683</td>\n",
       "      <td>0.140911</td>\n",
       "      <td>0.105236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.137873</td>\n",
       "      <td>0.006130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.517811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138118</td>\n",
       "      <td>0.625627</td>\n",
       "      <td>0.207374</td>\n",
       "      <td>0.378062</td>\n",
       "      <td>0.707339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.585989</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190926</td>\n",
       "      <td>0.674037</td>\n",
       "      <td>0.277845</td>\n",
       "      <td>0.499942</td>\n",
       "      <td>0.738961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.668395</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261726</td>\n",
       "      <td>0.740663</td>\n",
       "      <td>0.335816</td>\n",
       "      <td>0.569962</td>\n",
       "      <td>0.788177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964381</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994735</td>\n",
       "      <td>0.790831</td>\n",
       "      <td>0.989870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Activity           D1           D2           D3           D4  \\\n",
       "count  3751.000000  3751.000000  3751.000000  3751.000000  3751.000000   \n",
       "mean      0.542255     0.076948     0.592436     0.068142     0.038990   \n",
       "std       0.498278     0.079989     0.105860     0.078414     0.115885   \n",
       "min       0.000000     0.000000     0.282128     0.000000     0.000000   \n",
       "25%       0.000000     0.033300     0.517811     0.000000     0.000000   \n",
       "50%       1.000000     0.066700     0.585989     0.050000     0.000000   \n",
       "75%       1.000000     0.100000     0.668395     0.100000     0.000000   \n",
       "max       1.000000     1.000000     0.964381     0.950000     1.000000   \n",
       "\n",
       "                D5           D6           D7           D8           D9  ...  \\\n",
       "count  3751.000000  3751.000000  3751.000000  3751.000000  3751.000000  ...   \n",
       "mean      0.212112     0.686653     0.274713     0.455133     0.749517  ...   \n",
       "std       0.102592     0.078702     0.090017     0.162731     0.071702  ...   \n",
       "min       0.002630     0.137873     0.006130     0.000000     0.275590  ...   \n",
       "25%       0.138118     0.625627     0.207374     0.378062     0.707339  ...   \n",
       "50%       0.190926     0.674037     0.277845     0.499942     0.738961  ...   \n",
       "75%       0.261726     0.740663     0.335816     0.569962     0.788177  ...   \n",
       "max       1.000000     0.994735     0.790831     0.989870     1.000000  ...   \n",
       "\n",
       "             D1767        D1768        D1769        D1770        D1771  \\\n",
       "count  3751.000000  3751.000000  3751.000000  3751.000000  3751.000000   \n",
       "mean      0.026926     0.014663     0.013863     0.021861     0.015196   \n",
       "std       0.161889     0.120215     0.116938     0.146249     0.122348   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             D1772        D1773        D1774        D1775        D1776  \n",
       "count  3751.000000  3751.000000  3751.000000  3751.000000  3751.000000  \n",
       "mean      0.016796     0.012263     0.011730     0.020261     0.011197  \n",
       "std       0.128522     0.110074     0.107683     0.140911     0.105236  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
       "\n",
       "[8 rows x 1777 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# изучаем данные\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# смотрим на сбалансированность классов\n",
    "\n",
    "sns.countplot(data=data, x='Activity');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Вывод по данным:* Данные обработаны, нет текстовых данных, столбцы закодированы и нормализованы, классы сбалансированы."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем матрицу наблюдейни X и вектор ответов y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Activity'], axis=1)\n",
    "y = data['Activity']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделяем выборку на тренировочную и тестовую в соотношении 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация гиперпараметров модели"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе лог регрессия: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Создаем объект класса логистическая регрессия\n",
    "log_reg = linear_model.LogisticRegression()\n",
    "\n",
    "#Обучаем модель, минимизируя logloss\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "print('f1_score на тестовом наборе лог регрессия: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе случ лес: 0.83\n"
     ]
    }
   ],
   "source": [
    "#Создаем объект класса случайный лес\n",
    "rf = ensemble.RandomForestClassifier(random_state=42)\n",
    "\n",
    "#Обучаем модель\n",
    "rf.fit(X_train, y_train)\n",
    "#Выводим значения метрики \n",
    "y_test_pred = rf.predict(X_test)\n",
    "print('f1_score на тестовом наборе случ лес: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Подбор гиперпараметров с помощью **GridSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13.8 s\n",
      "Wall time: 1min 18s\n",
      "f1_score на тестовом наборе: 0.78\n",
      "Наилучшие значения гиперпараметров: {'penalty': 'l2', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Логистическая регрессия\n",
    "\n",
    "param_grid = {'penalty': ['l2', 'none'] ,\n",
    "              'solver': ['lbfgs', 'saga'],\n",
    "              }\n",
    "grid_search = GridSearchCV(\n",
    "        estimator=linear_model.LogisticRegression(\n",
    "        random_state=42\n",
    "    ), \n",
    "    param_grid=param_grid, \n",
    "    cv=5, # количество фолдов при кросс-валидации\n",
    "    n_jobs = -1\n",
    ")  \n",
    "%time grid_search.fit(X_train, y_train)\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))\n",
    "print(\"Наилучшие значения гиперпараметров: {}\".format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7.03 s\n",
      "Wall time: 4min 59s\n",
      "f1_score на тестовом наборе: 0.84\n",
      "Наилучшие значения гиперпараметров: {'max_depth': 22, 'min_samples_leaf': 5, 'n_estimators': 140}\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Случайный лес\n",
    "\n",
    "param_grid = {'n_estimators': list(range(80, 200, 30)),\n",
    "              'min_samples_leaf': [5],\n",
    "              'max_depth': list(np.linspace(20, 40, 10, dtype=int))\n",
    "              }\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "        estimator=ensemble.RandomForestClassifier(\n",
    "        random_state=42\n",
    "    ), \n",
    "    param_grid=param_grid, \n",
    "    cv=5, # количество фолдов при кросс-валидации\n",
    "    n_jobs = -1\n",
    ")  \n",
    "%time grid_search.fit(X_train, y_train)\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))\n",
    "print(\"Наилучшие значения гиперпараметров: {}\".format(grid_search.best_params_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Вывод:* обращаю внимание, что в методах уже присутствует встроенная кросс-валидация.<br><br>\n",
    "\n",
    "f1_score на тестовом наборе *лог регрессия*: 0.79 <br>\n",
    "ПОСЛЕ ПОДБОРА ГИПЕРПАРАМЕТРОВ GridSearchCV:  0.78 Метрика ухудшилась. Возможно мы не указали все возможные варианты в сетке параметров.<br> <br>\n",
    "\n",
    "f1_score на тестовом наборе *случ лес*: 0.83<br>\n",
    "ПОСЛЕ ПОДБОРА ГИПЕРПАРАМЕТРОВ GridSearchCV: 0.84 Метрику удалось немного улучшить."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Подбор гиперпараметров с помощью **RandomizedSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 4 is smaller than n_iter=50. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 12.7 s\n",
      "Wall time: 1min 3s\n",
      "f1_score на тестовом наборе: 0.78\n",
      "Наилучшие значения гиперпараметров: {'solver': 'saga', 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Логистическая регрессия\n",
    "\n",
    "param_distributions = {'penalty': ['l2', 'none'] ,\n",
    "              'solver': ['lbfgs', 'saga'],\n",
    "}\n",
    "            \n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=linear_model.LogisticRegression(random_state=42), \n",
    "    param_distributions=param_distributions, \n",
    "    cv=5, # количество фолдов при кросс-валидации\n",
    "    n_iter = 50, \n",
    "    n_jobs = -1\n",
    ")  \n",
    "%time random_search.fit(X_train, y_train) \n",
    "y_test_pred = random_search.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))\n",
    "print(\"Наилучшие значения гиперпараметров: {}\".format(random_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 40 is smaller than n_iter=50. Running 40 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6.56 s\n",
      "Wall time: 5min 47s\n",
      "f1_score на тестовом наборе: 0.84\n",
      "Наилучшие значения гиперпараметров: {'n_estimators': 140, 'min_samples_leaf': 5, 'max_depth': 22}\n"
     ]
    }
   ],
   "source": [
    "# 2.2.1 Случайный лес\n",
    "\n",
    "param_distributions = {'n_estimators': list(range(80, 200, 30)),\n",
    "              'min_samples_leaf': [5],\n",
    "              'max_depth': list(np.linspace(20, 40, 10, dtype=int))\n",
    "}\n",
    "            \n",
    "random_search_forest = RandomizedSearchCV(\n",
    "    estimator=ensemble.RandomForestClassifier(random_state=42), \n",
    "    param_distributions=param_distributions, \n",
    "    cv=5, # количество фолдов при кросс-валидации\n",
    "    n_iter = 50, \n",
    "    n_jobs = -1\n",
    ")  \n",
    "%time random_search_forest.fit(X_train, y_train) \n",
    "y_test_pred = random_search_forest.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))\n",
    "print(\"Наилучшие значения гиперпараметров: {}\".format(random_search_forest.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9.05 s\n",
      "Wall time: 9min 6s\n",
      "f1_score на тестовом наборе: 0.83\n",
      "Наилучшие значения гиперпараметров: {'n_estimators': 140, 'min_samples_leaf': 5, 'max_depth': 24, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "# 2.2.2 Случайный лес\n",
    "\n",
    "param_distributions = {'n_estimators': list(range(80, 200, 30)),\n",
    "              'min_samples_leaf': [5],\n",
    "              'max_depth': list(np.linspace(20, 40, 10, dtype=int)),\n",
    "              'criterion': ['gini', 'entropy']\n",
    "}\n",
    "            \n",
    "random_search_forest = RandomizedSearchCV(\n",
    "    estimator=ensemble.RandomForestClassifier(random_state=42), \n",
    "    param_distributions=param_distributions, \n",
    "    cv=5, # количество фолдов при кросс-валидации\n",
    "    n_iter = 50, \n",
    "    n_jobs = -1\n",
    ")  \n",
    "%time random_search_forest.fit(X_train, y_train) \n",
    "y_test_pred = random_search_forest.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))\n",
    "print(\"Наилучшие значения гиперпараметров: {}\".format(random_search_forest.best_params_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Вывод:* обращаю внимание, что в методах уже присутствует встроенная кросс-валидация.<br><br>\n",
    "\n",
    "f1_score на тестовом наборе *лог регрессия*: 0.79 <br>\n",
    "GridSearchCV:  0.78<br>\n",
    "RandomizedSearchCV: 0.78 Параметр не улучшился. Наилучшие значения гиперпараметров те же.<br> <br>\n",
    "\n",
    "f1_score на тестовом наборе *случ лес*: 0.83<br>\n",
    "GridSearchCV: 0.84<br>\n",
    "RandomizedSearchCV: 0.84 Метрика не изменилась, наилучшие параметры те же. При расширении сетки параметров, метрика ухудшается."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Подбор гиперпараметров с помощью **Hyperopt**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Логистическая регрессия\n",
    "\n",
    "space={'penalty': hp.choice('penalty', ['l2', 'none']),\n",
    "              'solver': hp.choice('solver', ['lbfgs', 'sag']),\n",
    "              'C': hp.uniform('C', 0.01, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "def hyperopt_lr(space, cv=5, X=X_train, y=y_train, random_state=random_state):\n",
    "    # функция получает комбинацию гиперпараметров в \"params\"\n",
    "    space = {'penalty': space['penalty'], \n",
    "             'solver': space['solver'], \n",
    "             'C': float(space['C'])\n",
    "    }\n",
    "  \n",
    "    # используем эту комбинацию для построения модели\n",
    "    model = linear_model.LogisticRegression(**space, random_state=random_state)\n",
    "\n",
    "    # обучаем модель\n",
    "    model.fit(X, y)\n",
    "    score = cross_val_score(model, X, y, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n",
    "    return {'loss': -score, 'params': space, 'status': STATUS_OK}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:12<10:22, 12.70s/trial, best loss: -0.7558431355295239]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:45<19:23, 24.24s/trial, best loss: -0.7729382445367267]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:52<12:56, 16.53s/trial, best loss: -0.7855726459575307]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [01:32<19:48, 25.85s/trial, best loss: -0.7855726459575307]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [01:42<14:58, 19.96s/trial, best loss: -0.7855726459575307]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [02:21<19:33, 26.68s/trial, best loss: -0.7855726459575307]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [03:00<21:56, 30.61s/trial, best loss: -0.7855726459575307]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [03:08<16:28, 23.53s/trial, best loss: -0.7855726459575307]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [03:16<12:45, 18.67s/trial, best loss: -0.7855726459575307]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [03:51<15:39, 23.49s/trial, best loss: -0.7855726459575307]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [03:58<12:03, 18.55s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [04:38<15:55, 25.14s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [04:47<12:32, 20.34s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [05:29<16:02, 26.74s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [06:13<18:38, 31.97s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [06:20<13:53, 24.52s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [07:00<15:55, 28.97s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [07:44<18:00, 33.76s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [08:27<18:43, 36.24s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [08:36<14:10, 28.34s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [08:47<11:05, 22.96s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [09:10<05:32, 12.80s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [09:26<04:07, 10.32s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [09:37<04:04, 10.61s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [09:47<03:47, 10.34s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [09:58<03:37, 10.37s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [10:07<03:23, 10.17s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [10:17<03:11, 10.07s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [10:27<02:59,  9.97s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [10:36<02:44,  9.66s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [10:46<02:35,  9.73s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [10:56<02:27,  9.84s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [11:05<02:14,  9.60s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [11:16<02:10, 10.07s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [11:26<02:00, 10.03s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [11:35<01:47,  9.74s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [11:44<01:34,  9.46s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [11:53<01:23,  9.26s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [12:02<01:15,  9.43s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [12:47<02:19, 19.88s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [12:57<01:42, 17.02s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [13:41<02:05, 25.02s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [13:51<01:22, 20.59s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [14:35<01:22, 27.49s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [14:44<00:44, 22.08s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [14:55<00:18, 18.63s/trial, best loss: -0.7866466124784772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [15:39<00:00, 18.79s/trial, best loss: -0.7866466124784772]\n",
      "Наилучшие значения гиперпараметров {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.05084775379720359}\n"
     ]
    }
   ],
   "source": [
    "# начинаем подбор гиперпараметров\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best=fmin(hyperopt_lr, \n",
    "          space=space, \n",
    "          algo=tpe.suggest,\n",
    "          max_evals=50,\n",
    "          trials=trials,\n",
    "          rstate=np.random.default_rng(random_state)\n",
    "         )\n",
    "print(\"Наилучшие значения гиперпараметров {}\".format(trials.best_trial['result']['params']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# рассчитаем точность для тестовой выборки\n",
    "model = linear_model.LogisticRegression(\n",
    "    random_state=random_state, \n",
    "    penalty=trials.best_trial['result']['params']['penalty'],\n",
    "    solver=trials.best_trial['result']['params']['solver'],\n",
    "    C=float(trials.best_trial['result']['params']['C'])\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Случайный лес\n",
    "\n",
    "space={'n_estimators': hp.quniform('n_estimators', 80, 200, 1),\n",
    "       'max_depth' : hp.quniform('max_depth', 15, 40, 1),\n",
    "       'min_samples_leaf': hp.quniform('min_samples_leaf', 2, 10, 1)\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "def hyperopt_rf(params, cv=5, X=X_train, y=y_train, random_state=random_state):\n",
    "    # функция получает комбинацию гиперпараметров в \"params\"\n",
    "    params = {'n_estimators': int(params['n_estimators']), \n",
    "              'max_depth': int(params['max_depth']), \n",
    "             'min_samples_leaf': int(params['min_samples_leaf'])\n",
    "              }\n",
    "  \n",
    "    # используем эту комбинацию для построения модели\n",
    "    model = ensemble.RandomForestClassifier(**params, random_state=random_state)\n",
    "\n",
    "    # обучаем модель\n",
    "    model.fit(X, y)\n",
    "    score = metrics.f1_score(y, model.predict(X))\n",
    "    \n",
    "    # кросс-валидация\n",
    "    score = cross_val_score(model, X, y, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n",
    "\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [17:43<00:00, 21.27s/trial, best loss: -0.8114539037371413]\n",
      "Наилучшие значения гиперпараметров {'max_depth': 22.0, 'min_samples_leaf': 2.0, 'n_estimators': 152.0}\n"
     ]
    }
   ],
   "source": [
    "# начинаем подбор гиперпараметров\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best=fmin(hyperopt_rf, # наша функция \n",
    "          space=space, # пространство гиперпараметров\n",
    "          algo=tpe.suggest, # алгоритм оптимизации, установлен по умолчанию, задавать необязательно\n",
    "          max_evals=50, # максимальное количество итераций\n",
    "          trials=trials, # логирование результатов\n",
    "          rstate=np.random.default_rng(random_state)# фиксируем для повторяемости результата\n",
    "         )\n",
    "print(\"Наилучшие значения гиперпараметров {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.83\n"
     ]
    }
   ],
   "source": [
    "# рассчитаем точность для тестовой выборки\n",
    "model = ensemble.RandomForestClassifier(\n",
    "    random_state=random_state, \n",
    "    n_estimators=int(best['n_estimators']),\n",
    "    max_depth=int(best['max_depth']),\n",
    "    min_samples_leaf=int(best['min_samples_leaf'])\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Вывод:* Модели обучены с помощью кросс-валидации<br>\n",
    "\n",
    "f1_score на тестовом наборе *лог регрессия*: 0.79 <br>\n",
    "GridSearchCV:  0.78 <br>\n",
    "RandomizedSearchCV: 0.78 <br>\n",
    "Hyperopt: 0.79 Метрика улучшилась.\n",
    "<br> <br>\n",
    "\n",
    "f1_score на тестовом наборе *случ лес*: 0.83<br>\n",
    "GridSearchCV: 0.84 <br>\n",
    "RandomizedSearchCV: 0.84 <br>\n",
    "Hyperopt: 0.83 Метрика осталась такой же на выбранных наилучших гиперпараметрах."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Подбор гиперпараметров с помощью **Optuna**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-05 19:08:40,798] A new study created in memory with name: LogisticRegression\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-05 19:08:50,765] Trial 0 finished with value: 0.7808219178082192 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.5075854368966629}. Best is trial 0 with value: 0.7808219178082192.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-05 19:08:52,431] Trial 1 finished with value: 0.7575757575757576 and parameters: {'penalty': 'none', 'solver': 'lbfgs', 'C': 0.8618301461218764}. Best is trial 0 with value: 0.7808219178082192.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-05 19:09:06,816] Trial 2 finished with value: 0.7894736842105263 and parameters: {'penalty': 'none', 'solver': 'sag', 'C': 0.6331556383115067}. Best is trial 2 with value: 0.7894736842105263.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-05 19:09:08,358] Trial 3 finished with value: 0.7575757575757576 and parameters: {'penalty': 'none', 'solver': 'lbfgs', 'C': 0.20990961574370343}. Best is trial 2 with value: 0.7894736842105263.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-05 19:09:09,913] Trial 4 finished with value: 0.7575757575757576 and parameters: {'penalty': 'none', 'solver': 'lbfgs', 'C': 0.7152451243211443}. Best is trial 2 with value: 0.7894736842105263.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-05 19:09:11,460] Trial 5 finished with value: 0.7575757575757576 and parameters: {'penalty': 'none', 'solver': 'lbfgs', 'C': 0.10485011245399585}. Best is trial 2 with value: 0.7894736842105263.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-05 19:09:21,532] Trial 6 finished with value: 0.7894736842105263 and parameters: {'penalty': 'none', 'solver': 'sag', 'C': 0.06346099365685585}. Best is trial 2 with value: 0.7894736842105263.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-05 19:09:36,854] Trial 7 finished with value: 0.7894736842105263 and parameters: {'penalty': 'none', 'solver': 'sag', 'C': 0.5236788303346651}. Best is trial 2 with value: 0.7894736842105263.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-05 19:09:38,326] Trial 8 finished with value: 0.7575757575757576 and parameters: {'penalty': 'none', 'solver': 'lbfgs', 'C': 0.9897689927157607}. Best is trial 2 with value: 0.7894736842105263.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2023-07-05 19:09:39,819] Trial 9 finished with value: 0.7575757575757576 and parameters: {'penalty': 'none', 'solver': 'lbfgs', 'C': 0.4387688160765148}. Best is trial 2 with value: 0.7894736842105263.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-05 19:09:54,056] Trial 10 finished with value: 0.782212086659065 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.7228910054092283}. Best is trial 2 with value: 0.7894736842105263.\n",
      "[I 2023-07-05 19:10:07,712] Trial 11 finished with value: 0.7977011494252874 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.011114393210518714}. Best is trial 11 with value: 0.7977011494252874.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-05 19:10:20,360] Trial 12 finished with value: 0.7909090909090909 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.2603367792369021}. Best is trial 11 with value: 0.7977011494252874.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-05 19:10:36,544] Trial 13 finished with value: 0.7909090909090909 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.25212338003782697}. Best is trial 11 with value: 0.7977011494252874.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-05 19:10:48,079] Trial 14 finished with value: 0.8064146620847653 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.0175223833502189}. Best is trial 14 with value: 0.8064146620847653.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-05 19:11:00,671] Trial 15 finished with value: 0.7926689576174113 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.053543153262350256}. Best is trial 14 with value: 0.8064146620847653.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-05 19:11:14,600] Trial 16 finished with value: 0.7954285714285714 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.04236103868544826}. Best is trial 14 with value: 0.8064146620847653.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-05 19:11:28,330] Trial 17 finished with value: 0.7936507936507937 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.16950958964411955}. Best is trial 14 with value: 0.8064146620847653.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-05 19:11:42,493] Trial 18 finished with value: 0.785876993166287 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.3275748680320627}. Best is trial 14 with value: 0.8064146620847653.\n",
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2023-07-05 19:11:56,046] Trial 19 finished with value: 0.7954545454545454 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.12226380236489487}. Best is trial 14 with value: 0.8064146620847653.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sveta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1.4 Логистическая регрессия\n",
    "\n",
    "random_state = 42\n",
    "def optuna_lr(trial):\n",
    "  # задаем пространства поиска гиперпараметров\n",
    "  penalty = trial.suggest_categorical('penalty', ['l2', 'none'])\n",
    "  solver = trial.suggest_categorical('solver', ['lbfgs', 'sag']) \n",
    "  C = trial.suggest_float('C',0.01,1)\n",
    "\n",
    "  # создаем модель\n",
    "  model = linear_model.LogisticRegression(penalty=penalty,\n",
    "                                          solver=solver,\n",
    "                                          C=C,\n",
    "                                          random_state=random_state\n",
    "  )\n",
    "  # обучаем модель\n",
    "  model.fit(X_train, y_train)\n",
    "  score = metrics.f1_score(y_test, model.predict(X_test))\n",
    "\n",
    "  return score\n",
    "\n",
    "# cоздаем объект исследования\n",
    "study = optuna.create_study(study_name=\"LogisticRegression\", direction=\"maximize\")\n",
    "# ищем лучшую комбинацию гиперпараметров n_trials раз\n",
    "study.optimize(optuna_lr, n_trials=20)\n",
    "\n",
    "\n",
    "# рассчитаем точность для тестовой выборки\n",
    "model = linear_model.LogisticRegression(**study.best_params,random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Случайный лес\n",
    "\n",
    "random_state = 42\n",
    "def optuna_rf(trial):\n",
    "  # задаем пространства поиска гиперпараметров\n",
    "  n_estimators = trial.suggest_int('n_estimators', 100, 200, 1)\n",
    "  max_depth = trial.suggest_int('max_depth', 10, 30, 1)\n",
    "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 10, 1)\n",
    "\n",
    "  # создаем модель\n",
    "  model = ensemble.RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                          max_depth=max_depth,\n",
    "                                          min_samples_leaf=min_samples_leaf,\n",
    "                                          random_state=random_state\n",
    "  )\n",
    "  # обучаем модель\n",
    "  model.fit(X_train, y_train)\n",
    "  score = metrics.f1_score(y_test, model.predict(X_test))\n",
    "\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-05 18:51:40,511] A new study created in memory with name: RandomForestClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-05 18:51:47,336] Trial 0 finished with value: 0.8329466357308585 and parameters: {'n_estimators': 171, 'max_depth': 20, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.8329466357308585.\n",
      "[I 2023-07-05 18:51:56,741] Trial 1 finished with value: 0.8283062645011601 and parameters: {'n_estimators': 200, 'max_depth': 15, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8329466357308585.\n",
      "[I 2023-07-05 18:52:04,799] Trial 2 finished with value: 0.8259860788863108 and parameters: {'n_estimators': 189, 'max_depth': 25, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.8329466357308585.\n",
      "[I 2023-07-05 18:52:10,026] Trial 3 finished with value: 0.8266360505166476 and parameters: {'n_estimators': 110, 'max_depth': 25, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.8329466357308585.\n",
      "[I 2023-07-05 18:52:16,983] Trial 4 finished with value: 0.8319814600231751 and parameters: {'n_estimators': 151, 'max_depth': 21, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.8329466357308585.\n",
      "[I 2023-07-05 18:52:23,295] Trial 5 finished with value: 0.8350634371395618 and parameters: {'n_estimators': 109, 'max_depth': 27, 'min_samples_leaf': 3}. Best is trial 5 with value: 0.8350634371395618.\n",
      "[I 2023-07-05 18:52:30,195] Trial 6 finished with value: 0.8277456647398843 and parameters: {'n_estimators': 190, 'max_depth': 15, 'min_samples_leaf': 9}. Best is trial 5 with value: 0.8350634371395618.\n",
      "[I 2023-07-05 18:52:38,237] Trial 7 finished with value: 0.822843822843823 and parameters: {'n_estimators': 198, 'max_depth': 26, 'min_samples_leaf': 8}. Best is trial 5 with value: 0.8350634371395618.\n",
      "[I 2023-07-05 18:52:47,298] Trial 8 finished with value: 0.8325581395348837 and parameters: {'n_estimators': 192, 'max_depth': 22, 'min_samples_leaf': 2}. Best is trial 5 with value: 0.8350634371395618.\n",
      "[I 2023-07-05 18:52:52,651] Trial 9 finished with value: 0.8191365227537922 and parameters: {'n_estimators': 146, 'max_depth': 21, 'min_samples_leaf': 8}. Best is trial 5 with value: 0.8350634371395618.\n",
      "[I 2023-07-05 18:52:58,137] Trial 10 finished with value: 0.825287356321839 and parameters: {'n_estimators': 102, 'max_depth': 30, 'min_samples_leaf': 2}. Best is trial 5 with value: 0.8350634371395618.\n",
      "[I 2023-07-05 18:53:04,408] Trial 11 finished with value: 0.8186046511627907 and parameters: {'n_estimators': 160, 'max_depth': 11, 'min_samples_leaf': 5}. Best is trial 5 with value: 0.8350634371395618.\n",
      "[I 2023-07-05 18:53:10,107] Trial 12 finished with value: 0.8323699421965319 and parameters: {'n_estimators': 126, 'max_depth': 17, 'min_samples_leaf': 4}. Best is trial 5 with value: 0.8350634371395618.\n",
      "[I 2023-07-05 18:53:18,480] Trial 13 finished with value: 0.8329466357308585 and parameters: {'n_estimators': 172, 'max_depth': 30, 'min_samples_leaf': 3}. Best is trial 5 with value: 0.8350634371395618.\n",
      "[I 2023-07-05 18:53:23,857] Trial 14 finished with value: 0.8250289687137891 and parameters: {'n_estimators': 129, 'max_depth': 18, 'min_samples_leaf': 6}. Best is trial 5 with value: 0.8350634371395618.\n",
      "[I 2023-07-05 18:53:32,449] Trial 15 finished with value: 0.8244803695150116 and parameters: {'n_estimators': 171, 'max_depth': 27, 'min_samples_leaf': 7}. Best is trial 5 with value: 0.8350634371395618.\n",
      "[I 2023-07-05 18:53:38,903] Trial 16 finished with value: 0.8387096774193548 and parameters: {'n_estimators': 135, 'max_depth': 24, 'min_samples_leaf': 4}. Best is trial 16 with value: 0.8387096774193548.\n",
      "[I 2023-07-05 18:53:44,902] Trial 17 finished with value: 0.8394495412844036 and parameters: {'n_estimators': 123, 'max_depth': 28, 'min_samples_leaf': 4}. Best is trial 17 with value: 0.8394495412844036.\n",
      "[I 2023-07-05 18:53:51,256] Trial 18 finished with value: 0.8350634371395618 and parameters: {'n_estimators': 131, 'max_depth': 23, 'min_samples_leaf': 4}. Best is trial 17 with value: 0.8394495412844036.\n",
      "[I 2023-07-05 18:53:57,701] Trial 19 finished with value: 0.8341013824884792 and parameters: {'n_estimators': 139, 'max_depth': 28, 'min_samples_leaf': 5}. Best is trial 17 with value: 0.8394495412844036.\n"
     ]
    }
   ],
   "source": [
    "# cоздаем объект исследования\n",
    "study = optuna.create_study(study_name=\"RandomForestClassifier\", direction=\"maximize\")\n",
    "# ищем лучшую комбинацию гиперпараметров n_trials раз\n",
    "study.optimize(optuna_rf, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.84\n"
     ]
    }
   ],
   "source": [
    "# рассчитаем точность для тестовой выборки\n",
    "model = ensemble.RandomForestClassifier(**study.best_params,random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print('f1_score на тестовом наборе: {:.2f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Вывод:*<br>\n",
    "\n",
    "f1_score на тестовом наборе *лог регрессия*: 0.79 <br>\n",
    "GridSearchCV:  0.78 <br>\n",
    "RandomizedSearchCV: 0.78 <br>\n",
    "Hyperopt: 0.79<br>\n",
    "Optuna: 0.81 Метод подбора гиперпараметров Optuna показал наивысшую метрику при логистической регрессии и модели случайношл леса.\n",
    "<br> <br>\n",
    "\n",
    "f1_score на тестовом наборе *случ лес*: 0.83<br>\n",
    "GridSearchCV: 0.84 <br>\n",
    "RandomizedSearchCV: 0.84 <br>\n",
    "Hyperopt: 0.83<br>\n",
    "Optuna: 0.84 Метод подбора гиперпараметров Optuna показал наивысшую метрику."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
